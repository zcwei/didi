{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load district hashmap first\n",
    "def loadCluster(path):\n",
    "    allFiles = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    dlist=[]\n",
    "    col_names=['district_hash','district_id'] \n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_table(file_, sep='\\t', names=col_names)\n",
    "        dlist.append(df)\n",
    "    df= pd.concat(dlist,ignore_index=True)\n",
    "    return dict(zip(df.district_hash, df.district_id))\n",
    "\n",
    "\n",
    "def loadOrders(path):\n",
    "    allFiles = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    dlist=[]\n",
    "    col_names=['order_id','driver_id','passenger_id','start_district_id',\n",
    "               'dest_district_id', 'Price','Time'] \n",
    "    #toload=1\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_table(file_, sep='\\t',parse_dates=[6], names=col_names)\n",
    "        df=df.replace({'start_district_id':cluster})\n",
    "        df=df.replace({'dest_district_id':cluster})\n",
    "        dlist.append(df)\n",
    "    df= pd.concat(dlist,ignore_index=True)\n",
    "    df['time_slot']=[(t.hour*60+t.minute)/10+1 for t in df.Time]\n",
    "    df['min'] = [(t.hour*60+t.minute)+1 for t in df.Time]\n",
    "    df= df[df.Time.dt.date!=datetime.date(2016,1,1)]\n",
    "    df['date'] = df['Time'].dt.date\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    return df\n",
    "    \n",
    "def loadWeather(path):\n",
    "    allFiles = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    dlist=[]\n",
    "    col_names=['Time','Weather','Temperature','PM25'] \n",
    "    #toload=1\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_table(file_, sep='\\t',parse_dates=[0], names=col_names)\n",
    "        df['time_slot']=[(t.hour*60+t.minute)/10+1 for t in df.Time]\n",
    "        dlist.append(df)\n",
    "\n",
    "    df= pd.concat(dlist,ignore_index=True)\n",
    "    df= df[df.Time.dt.date!=datetime.date(2016,1,1)]\n",
    "    df['date'] = df['Time'].dt.date\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    return df\n",
    "    \n",
    "def loadTraffic(path):\n",
    "    allFiles = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    dlist=[]\n",
    "    col_names=['district_id','lv1','lv2','lv3','lv4','Time'] \n",
    "    #toload=1\n",
    "    def myfun(s):\n",
    "        return int(s[2:])\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_table(file_, sep='\\t',parse_dates=[5], names=col_names,\n",
    "                           converters={1:myfun,2:myfun,3:myfun,4:myfun})\n",
    "        df=df.replace({'district_id':cluster})\n",
    "        dlist.append(df)\n",
    "        df['time_slot']=[(t.hour*60+t.minute)/10+1 for t in df.Time]\n",
    "\n",
    "    df = pd.concat(dlist,ignore_index=True)\n",
    "    df= df[df.Time.dt.date!=datetime.date(2016,1,1)]\n",
    "    df['date'] = df['Time'].dt.date\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # load cluster map\n",
    "path=\"season_1/training_data/cluster_map\"\n",
    "cluster = loadCluster(path)\n",
    "\n",
    "# load orders data\n",
    "path=\"season_1/training_data/order_data\"\n",
    "orders_train=loadOrders(path)\n",
    "\n",
    "\n",
    "# load traffic data\n",
    "path=\"season_1/training_data/traffic_data\"\n",
    "traffic_train=loadTraffic(path)\n",
    "\n",
    "#load weather data\n",
    "path=\"season_1/training_data/weather_data\"\n",
    "weather_train=loadWeather(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeSlotData(data, test = False):\n",
    "    index_col = ['date','time_slot']\n",
    "    data['date'] = data['Time'].dt.date\n",
    "    grouped =data.groupby(index_col , as_index = False)\n",
    "    time= pd.DataFrame()\n",
    "    time['date'] = grouped.count()['date']\n",
    "    time['time_slot'] = grouped.count()['time_slot']\n",
    "    time.date = pd.to_datetime(time.date)\n",
    "    time.sort(['date', 'time_slot'], inplace =True)\n",
    "    if(test):\n",
    "        a = time.iloc[::3, :]\n",
    "        b = a.copy(True)\n",
    "        b.time_slot = b.time_slot+3\n",
    "        time = time.append(b)\n",
    "        time.sort(['date', 'time_slot'], inplace =True)\n",
    "    return time\n",
    "\n",
    "def preparetimeSlotDistrict(data):\n",
    "    timeSlotDistrict = pd.concat([timeSlot for t in range(1,67)],ignore_index=True)\n",
    "    timeSlotDistrict['start_district_id']=[i for i in range(1,67) for j in range((timeSlot).shape[0])]\n",
    "    return timeSlotDistrict\n",
    "\n",
    "def prepareWeatherData(data, timeSlot, testData =False):\n",
    "    temp = data.copy(deep=True)\n",
    "    temp.time_slot = temp.time_slot.astype(int)\n",
    "    temp.drop(['Time'],axis = 1, inplace=True)\n",
    "    temp.drop_duplicates(['date', 'time_slot'], take_last = True, inplace=True)\n",
    "    temp = pd.merge(timeSlot, temp, on = ['time_slot', 'date'], how = 'left')\n",
    "    temp.fillna(method='ffill', limit=10, inplace =True)\n",
    "    temp.fillna(method='bfill', limit=10, inplace =True)\n",
    "    return temp \n",
    "\n",
    "def prepareTrafficData(data, timeSlotDistrict, test = False):\n",
    "    temp = data.copy(deep=True)\n",
    "    temp.time_slot = temp.time_slot.astype(int)\n",
    "    temp.district_id = temp.district_id.astype(int)\n",
    "    temp.sort(['district_id', 'Time'], inplace = True)\n",
    "    temp.drop(['Time'], axis = 1, inplace= True)\n",
    "    temp.rename(columns={'district_id':'start_district_id'}, inplace=True)\n",
    "    temp = pd.merge(timeSlotDistrict, temp, on = ['time_slot', 'date', 'start_district_id'], how = 'left')\n",
    "    \n",
    "    temp.sort(['date', 'start_district_id','time_slot'], inplace = True)\n",
    "    if (test != True):\n",
    "        temp.fillna(method='bfill', limit=2, inplace =True)\n",
    "        temp = temp.fillna({'lv1':  int(temp.lv1.mean()), 'lv2':  int(temp.lv2.mean()), \n",
    "                            'lv3': int(temp.lv3.mean()), 'lv4': int(temp.lv4.mean())})\n",
    "    temp['lv1_pect'] = temp.lv1/(temp.lv1+temp.lv2+temp.lv3+temp.lv4) \n",
    "    temp['lv2_pect'] = temp.lv2/(temp.lv1+temp.lv2+temp.lv3+temp.lv4) \n",
    "    temp['lv3_pect'] = temp.lv3/(temp.lv1+temp.lv2+temp.lv3+temp.lv4) \n",
    "    temp['lv4_pect'] = temp.lv4/(temp.lv1+temp.lv2+temp.lv3+temp.lv4) \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timeSlot = timeSlotData(orders_train)\n",
    "#weather data process\n",
    "weather = prepareWeatherData(weather_train, timeSlot)\n",
    "temp = weather[weather.date == datetime.date(2016,1,21)]\n",
    "weather.fillna({'Weather': 4, 'PM25': int((temp.PM25.mean()+100)/2), 'Temperature': int(temp.Temperature.mean())}, inplace=True)\n",
    "print(weather.isnull().any())\n",
    "\n",
    "##traffic data process \n",
    "timeSlotDistrict = preparetimeSlotDistrict(timeSlot)\n",
    "traffic = prepareTrafficData(traffic_train, timeSlotDistrict)\n",
    "print(traffic.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareOrderData(orders, weather, traffic, timeSlotDistrict, test = False):\n",
    "    index_col = ['date','time_slot', 'start_district_id']\n",
    "    grouped = orders.groupby(index_col)\n",
    "    data = pd.DataFrame()\n",
    "    data['demand']=grouped.count()['order_id']\n",
    "    data['supply']=grouped.count()['driver_id']\n",
    "    data = data.reset_index()\n",
    "    data['weekday'] = [t.isoweekday() for t in data.date]\n",
    "    data['gap']=data['demand']-data['supply']\n",
    "    data.date = pd.to_datetime(data.date)\n",
    "    data.start_district_id = data.start_district_id.astype(int)\n",
    "    data = pd.merge(timeSlotDistrict, data, on = ['time_slot', 'date', 'start_district_id'], how = 'left')\n",
    "    if(test):\n",
    "        data.fillna(method='ffill', limit=1, inplace =True)\n",
    "    \n",
    "    index_col = ['date','time_slot', 'start_district_id', 'min']\n",
    "    grouped = orders.groupby(index_col)\n",
    "    data_permin = pd.DataFrame()\n",
    "    data_permin['demand_per_min']=grouped.count()['order_id']\n",
    "    data_permin['supply_per_min']=grouped.count()['driver_id']\n",
    "    data_permin = data_permin.reset_index()\n",
    "    data_permin['weekday'] = [t.isoweekday() for t in data_permin.date]\n",
    "    data_permin['gap_per_min']=data_permin['demand_per_min']-data_permin['supply_per_min']\n",
    "    data_permin.date = pd.to_datetime(data_permin.date)\n",
    "    data_permin.start_district_id = data_permin.start_district_id.astype(int)\n",
    "    if 'weekday' in data_permin.columns:\n",
    "        data_permin.drop(['weekday'], axis= 1, inplace=True)\n",
    "    \n",
    "    total_data = data.copy(True)\n",
    "    total_data.date = pd.to_datetime(total_data.date)\n",
    "    total_data.start_district_id = total_data.start_district_id.astype(int)\n",
    "    col = ['date', 'gap', 'weekday', 'time_slot', 'start_district_id', 'demand', 'supply']\n",
    "    total_data = total_data[col]\n",
    "    key_col = ['date','time_slot']\n",
    "    total_data = pd.merge(total_data, weather, on = key_col, how = 'left' )\n",
    "    key_col = ['date','time_slot', 'start_district_id']\n",
    "    total_data = pd.merge(total_data, traffic, on = key_col, how = 'left')\n",
    "    \n",
    "    key_col = ['date','time_slot', 'start_district_id', 'min']\n",
    "    for i in range(1,11):\n",
    "        total_data['min'] = (total_data.time_slot-1)*10+i\n",
    "        total_data =  pd.merge(total_data, data_permin, on = key_col, how = 'left')\n",
    "        min_str = str(i)\n",
    "        col_dict = {'demand_per_min':'demand_min_'+ min_str, 'supply_per_min': 'supply_min_'+min_str, 'gap_per_min': 'gap_min_'+min_str}\n",
    "        total_data.rename(columns=col_dict, inplace=True)\n",
    "    \n",
    "    total_data.fillna(0, inplace = True)\n",
    "    total_data.sort(['date', 'start_district_id', 'time_slot'], inplace=True)\n",
    "    \n",
    "    #move t-1, t-2 and t-3 predictor to the same line in order to predict t0 gap \n",
    "    leftTable = total_data[['date', 'gap', 'weekday', 'time_slot', 'start_district_id']]\n",
    "    pass_1 = total_data.drop(['weekday'], axis = 1)\n",
    "    pass_1['time_slot'] = pass_1['time_slot']+1\n",
    "    pass_2 = total_data.drop(['weekday'], axis = 1)\n",
    "    pass_2['time_slot'] = pass_2['time_slot']+2\n",
    "    pass_3 = total_data.drop(['weekday'], axis = 1)\n",
    "    pass_3['time_slot'] = pass_3['time_slot']+3\n",
    "\n",
    "    result = pd.merge(leftTable, pass_1, on = ['date', 'time_slot', 'start_district_id'], suffixes=('', '_t_1'), how = 'left')\n",
    "    result = pd.merge(result, pass_2, on = ['date', 'time_slot', 'start_district_id'], suffixes=('', '_t_2'), how = 'left')\n",
    "    result = pd.merge(result, pass_3, on = ['date', 'time_slot', 'start_district_id'], suffixes=('', '_t_3'), how = 'left')\n",
    "\n",
    "    result = result[(result.time_slot != 1) & (result.time_slot != 2) & (result.time_slot != 3) ]\n",
    "    result.drop(['min'], axis=1, inplace =True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = prepareOrderData(orders_train, weather, traffic, timeSlotDistrict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import  cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import tensorflow as tf\n",
    "from operator import itemgetter\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##custom scoring function \n",
    "def mape(estimator, X, y, offset = 0):\n",
    "    y_pred = estimator.predict(X)\n",
    "    df = pd.DataFrame()\n",
    "    df['y'] = y\n",
    "    df['y_pred'] = y_pred - offset\n",
    "    df = df[X.time_slot >= 46]\n",
    "    df = df.round()\n",
    "    df.y_pred[df.y_pred <= 0] = 1  ##need set the value to at least 1 \n",
    "    dim = df.shape[0]\n",
    "    df = df[df.y != 0]\n",
    "    return ((np.sum(np.abs(df.y-df.y_pred)/df.y)/dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(grid_scores, n_top=3):\n",
    "    \"\"\"Report top n_top parameters settings, default n_top=3.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    grid_scores -- output from grid or random search\n",
    "    n_top -- how many to report, of top models\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_params -- [dict] top parameter settings found in\n",
    "                  search\n",
    "    \"\"\"\n",
    "    top_scores = sorted(grid_scores,\n",
    "                        key=itemgetter(1),\n",
    "                        reverse=False)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print((\"Mean validation score: \"\n",
    "               \"{0:.3f} (std: {1:.3f})\").format(\n",
    "               score.mean_validation_score,\n",
    "               np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "    return top_scores[0].parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###search for the best parameters\n",
    "def run_randomsearch(X, y, clf, para_dist, cv=5,\n",
    "                     n_iter_search=20):\n",
    "    \"\"\"Run a random search for best Decision Tree parameters.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    X -- features\n",
    "    y -- targets (classes)\n",
    "    cf -- scikit-learn Decision Tree\n",
    "    param_dist -- [dict] list, distributions of parameters\n",
    "                  to sample\n",
    "    cv -- fold of cross-validation, default 5\n",
    "    n_iter_search -- number of random parameter sets to try,\n",
    "                     default 20.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_params -- [dict] from report()\n",
    "    \"\"\"\n",
    "    random_search = RandomizedSearchCV(clf,\n",
    "                        param_distributions=param_dist,\n",
    "                        n_iter=n_iter_search, \n",
    "                        scoring = mape,\n",
    "                        n_jobs = 25)\n",
    "\n",
    "    start = time()\n",
    "    random_search.fit(X, y)\n",
    "    print((\"\\nRandomizedSearchCV took {:.2f} seconds \"\n",
    "           \"for {:d} candidates parameter settings.\").format((time() - start), n_iter_search))\n",
    "\n",
    "    top_params = report(random_search.grid_scores_, 5)\n",
    "    return  top_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_train = train_data[(train_data.date !=  datetime.date(2016,1,19)) & (train_data.date !=  datetime.date(2016,1,16)) ]\n",
    "dat_valid =  train_data[(train_data.date ==  datetime.date(2016,1,19)) | (train_data.date ==  datetime.date(2016,1,16)) ]\n",
    "\n",
    "print('train', dat_train.shape)\n",
    "print('validate', dat_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= dat_train.drop(['gap', 'date'], axis = 1)\n",
    "y =dat_train.gap\n",
    "x_valid= dat_valid.drop(['gap', 'date'], axis = 1)\n",
    "y_valid =dat_valid.gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "x_1= train_data.drop(['gap', 'date'], axis = 1)\n",
    "y_1 =train_data.gap\n",
    "params = {'n_estimators': 2000, 'max_depth': 4, 'min_samples_split': 5, 'learning_rate': 0.001, 'loss': 'lad'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "clf.fit(x_1, y_1)\n",
    "\n",
    "mape(clf, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mape(clf, x_valid, y_valid, 0.5))\n",
    "print(mape(clf, x_valid, y_valid, 1))\n",
    "print(mape(clf, x_valid, y_valid, 1.5))\n",
    "print(mape(clf, x_valid, y_valid, 2))\n",
    "print(mape(clf, x_valid, y_valid, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##optimize the parameter of the gbm \n",
    "param_dist = {\"n_estimators\": randint(100, 2000),\n",
    "              \"max_depth\": randint(1, 5),\n",
    "              \"min_samples_split\": randint(1,20),\n",
    "              \"learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "              \"loss\": 'lad'\n",
    "             }\n",
    "\n",
    "gbm_model = ensemble.GradientBoostingRegressor()\n",
    "gbm_model_search = run_randomsearch(x_1, y_1, gbm_model, param_dist, cv=5, n_iter_search=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#develop model for different region\n",
    "def developmodelForRegion(region_id, repeatSampleTimes, giveModel = False, model = None, ntops=1, test = False, testData=None):\n",
    "    #develop model for two region region 1\n",
    "    train = train_data[(train_data.date !=  datetime.date(2016,1,19)) & (train_data.date !=  datetime.date(2016,1,16)) ]\n",
    "    valid =  train_data[(train_data.date ==  datetime.date(2016,1,19)) | (train_data.date ==  datetime.date(2016,1,16)) ]\n",
    "    train = train[train.start_district_id==region_id] \n",
    "    x_valid= valid[valid.start_district_id==region_id].drop(['gap', 'date'], axis = 1)\n",
    "    y_valid =valid[valid.start_district_id==region_id].gap\n",
    "    scores = dict()\n",
    "    model_dict = dict()\n",
    "    if(giveModel == True):\n",
    "        gbm = model\n",
    "        for m in [0, 0.5, 1, 1.5, 2.0, 2.5, 3.0]: \n",
    "            result = mape(gbm, x_valid, y_valid, m)\n",
    "            #print('offset {0:.1f} {1:.5f}'.format (m, result))\n",
    "            key = str(region_id) + '-' + '0' + '-' + str(m)\n",
    "            scores[key] = result\n",
    "            model_dict[key] = gbm\n",
    "    \n",
    "    duplicate = train[(train.gap<=5) & (train.gap>= 1) ]\n",
    "    duplicate_copy = duplicate.copy(True)\n",
    "\n",
    "    for i in range(1, repeatSampleTimes):\n",
    "        dat_trian_with_duplicates =  train.copy(True)   \n",
    "        for j in range(1, i):\n",
    "            dat_trian_with_duplicates = dat_trian_with_duplicates.append(duplicate_copy)\n",
    "\n",
    "        x = dat_trian_with_duplicates.drop(['gap', 'date'], axis = 1)\n",
    "        y = dat_trian_with_duplicates.gap\n",
    "        params = {'n_estimators': 2000, 'max_depth': 4, 'min_samples_split': 5, 'learning_rate': 0.001, 'loss': 'lad'}\n",
    "        gbm = ensemble.GradientBoostingRegressor(**params)\n",
    "        gbm.fit(x, y)\n",
    "\n",
    "        for m in [0, 0.5, 1, 1.5, 2.0]: \n",
    "            result = mape(gbm, x_valid, y_valid, m)\n",
    "            #print('offset {0:.1f} {1:.5f}'.format (m, result))\n",
    "            key = str(region_id) + '-' + str(i) + '-' + str(m)\n",
    "            scores[key] = result\n",
    "            model_dict[key] = gbm\n",
    "        del gbm\n",
    "        del dat_trian_with_duplicates\n",
    "    \n",
    "    top_scores = sorted(scores.items(), key=itemgetter(1), reverse=False)[:ntops]\n",
    "    print(top_scores)\n",
    "    if(test == False):\n",
    "        return\n",
    "    else:\n",
    "        key = top_scores[0][0]\n",
    "        finalModel = model_dict[key]\n",
    "        testData_copy = testData.copy(True)\n",
    "        x_test = testData_copy.drop(['gap', 'date'], axis = 1)\n",
    "        result = finalModel.predict(x_test)\n",
    "        offset = float(key.split('-')[2])\n",
    "        result = result-offset\n",
    "        result =result.round()\n",
    "        result[result <= 0] = 1 \n",
    "        final_result = pd.DataFrame(testData_copy)\n",
    "        final_result['gap'] = result\n",
    "        final_result.sort(['date', 'time_slot', 'start_district_id'], inplace= True)\n",
    "        combinedTimeDistrict = lambda x: (x.date.strftime('%Y-%m-%d'))+'-'+str(x.time_slot)\n",
    "        final_result['Time_district'] = final_result.apply(combinedTimeDistrict, axis=1)\n",
    "        final_result.to_csv('result_'+str(region_id)+'.csv',columns=['start_district_id', 'Time_district', 'gap'], header = False, index =False)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "developmodelForRegion(7,1, True, clf, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from threading import Thread, current_thread\n",
    "threads = []\n",
    "\n",
    "for i in range(5, 8):\n",
    "    thread = Thread(\n",
    "        name=str(i),\n",
    "        target=developmodelForRegion,\n",
    "        args=(i, 4, True, clf)\n",
    "    )\n",
    "    \n",
    "    threads.append(thread)\n",
    "\n",
    "for i_thread, thread in enumerate(threads):\n",
    "    thread.start()\n",
    "\n",
    "for i_thread, thread in enumerate(threads):\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_train_2 = dat_train[dat_train.start_district_id == 2] \n",
    "x_valid_2= dat_valid[dat_valid.start_district_id == 2].drop(['gap', 'date'], axis = 1)\n",
    "y_valid_2 =dat_valid[dat_valid.start_district_id == 2].gap\n",
    "    \n",
    "x_2 = dat_train_2.drop(['gap', 'date'], axis = 1)\n",
    "y_2 = dat_train_2.gap\n",
    "params = {'n_estimators': 1000, 'max_depth': 4, 'min_samples_split': 5, 'learning_rate': 0.001, 'loss': 'lad'}\n",
    "clf_2 = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "clf_2.fit(x_2, y_2)\n",
    "print(mape(clf_2, x_valid_2, y_valid_2, 0))\n",
    "print(mape(clf_2, x_valid_2, y_valid_2, 0.5))\n",
    "print(mape(clf_2, x_valid_2, y_valid_2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mape(clf, x_valid_2, y_valid_2, 0))\n",
    "print(mape(clf, x_valid_2, y_valid_2, 0.5))\n",
    "print(mape(clf, x_valid_2, y_valid_2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Test data prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # load cluster map\n",
    "path=\"season_1/test_set_2/cluster_map\"\n",
    "cluster = loadCluster(path)\n",
    "\n",
    "# load orders data\n",
    "path=\"season_1/test_set_2/order_data\"\n",
    "orders_test=loadOrders(path)\n",
    "\n",
    "\n",
    "# load traffic data\n",
    "path=\"season_1/test_set_2/traffic_data\"\n",
    "traffic_test=loadTraffic(path)\n",
    "\n",
    "#load weather data\n",
    "path=\"season_1/test_set_2/weather_data\"\n",
    "weather_test=loadWeather(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timeSlot = timeSlotData(orders_test, True)\n",
    "#weather data process\n",
    "weather = prepareWeatherData(weather_test, timeSlot)\n",
    "print(weather.isnull().any())\n",
    "\n",
    "##traffic data process \n",
    "timeSlotDistrict = preparetimeSlotDistrict(timeSlot)\n",
    "traffic = prepareTrafficData(traffic_test, timeSlotDistrict, True)\n",
    "print(traffic.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = prepareOrderData(orders_test, weather, traffic, timeSlotDistrict, True)\n",
    "temp = test_data.iloc[3::4, :]\n",
    "test_data = temp.copy(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame(test_data)\n",
    "final_result['gap'] = result\n",
    "final_result.sort(['date', 'time_slot', 'start_district_id'], inplace= True)\n",
    "combinedTimeDistrict = lambda x: (x.date.strftime('%Y-%m-%d'))+'-'+str(x.time_slot)\n",
    "    \n",
    "final_result['Time_district'] = final_result.apply(combinedTimeDistrict, axis=1)\n",
    "final_result.to_csv('result_2.csv',columns=['start_district_id', 'Time_district', 'gap'], header = False, index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threads = []\n",
    "\n",
    "for i in range(5, 8):\n",
    "    thread = Thread(\n",
    "        name=str(i),\n",
    "        target=developmodelForRegion,\n",
    "        args=(i, 4, True, clf, 1, True, test_data[test_data.start_district_id==i])\n",
    "    )\n",
    "    \n",
    "    threads.append(thread)\n",
    "\n",
    "for i_thread, thread in enumerate(threads):\n",
    "    thread.start()\n",
    "\n",
    "for i_thread, thread in enumerate(threads):\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'data.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_data': train_data,\n",
    "    'clf': clf,\n",
    "    'test_data': test_data\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
