{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load district hashmap first\n",
    "def loadCluster(path):\n",
    "    allFiles = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    dlist=[]\n",
    "    col_names=['district_hash','district_id'] \n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_table(file_, sep='\\t', names=col_names)\n",
    "        dlist.append(df)\n",
    "    df= pd.concat(dlist,ignore_index=True)\n",
    "    return dict(zip(df.district_hash, df.district_id))\n",
    "\n",
    "\n",
    "def loadOrders(path):\n",
    "    allFiles = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    dlist=[]\n",
    "    col_names=['order_id','driver_id','passenger_id','start_district_id',\n",
    "               'dest_district_id', 'Price','Time'] \n",
    "    #toload=1\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_table(file_, sep='\\t',parse_dates=[6], names=col_names)\n",
    "        df=df.replace({'start_district_id':cluster})\n",
    "        df=df.replace({'dest_district_id':cluster})\n",
    "        dlist.append(df)\n",
    "    df= pd.concat(dlist,ignore_index=True)\n",
    "    df['time_slot']=[(t.hour*60+t.minute)/10+1 for t in df.Time]\n",
    "    df['min'] = [(t.hour*60+t.minute)+1 for t in df.Time]\n",
    "    df= df[df.Time.dt.date!=datetime.date(2016,1,1)]\n",
    "    df['date'] = df['Time'].dt.date\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    return df\n",
    "    \n",
    "def loadWeather(path):\n",
    "    allFiles = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    dlist=[]\n",
    "    col_names=['Time','Weather','Temperature','PM25'] \n",
    "    #toload=1\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_table(file_, sep='\\t',parse_dates=[0], names=col_names)\n",
    "        df['time_slot']=[(t.hour*60+t.minute)/10+1 for t in df.Time]\n",
    "        dlist.append(df)\n",
    "\n",
    "    df= pd.concat(dlist,ignore_index=True)\n",
    "    df= df[df.Time.dt.date!=datetime.date(2016,1,1)]\n",
    "    df['date'] = df['Time'].dt.date\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    return df\n",
    "    \n",
    "def loadTraffic(path):\n",
    "    allFiles = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    dlist=[]\n",
    "    col_names=['district_id','lv1','lv2','lv3','lv4','Time'] \n",
    "    #toload=1\n",
    "    def myfun(s):\n",
    "        return int(s[2:])\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_table(file_, sep='\\t',parse_dates=[5], names=col_names,\n",
    "                           converters={1:myfun,2:myfun,3:myfun,4:myfun})\n",
    "        df=df.replace({'district_id':cluster})\n",
    "        dlist.append(df)\n",
    "        df['time_slot']=[(t.hour*60+t.minute)/10+1 for t in df.Time]\n",
    "\n",
    "    df = pd.concat(dlist,ignore_index=True)\n",
    "    df= df[df.Time.dt.date!=datetime.date(2016,1,1)]\n",
    "    df['date'] = df['Time'].dt.date\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # load cluster map\n",
    "path=\"season_1/training_data/cluster_map\"\n",
    "cluster = loadCluster(path)\n",
    "\n",
    "# load orders data\n",
    "path=\"season_1/training_data/order_data\"\n",
    "orders_train=loadOrders(path)\n",
    "\n",
    "\n",
    "# load traffic data\n",
    "path=\"season_1/training_data/traffic_data\"\n",
    "traffic_train=loadTraffic(path)\n",
    "\n",
    "#load weather data\n",
    "path=\"season_1/training_data/weather_data\"\n",
    "weather_train=loadWeather(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeSlotData(data):\n",
    "    index_col = ['date','time_slot']\n",
    "    data['date'] = data['Time'].dt.date\n",
    "    grouped =data.groupby(index_col , as_index = False)\n",
    "    time= pd.DataFrame()\n",
    "    time['date'] = grouped.count()['date']\n",
    "    time['time_slot'] = grouped.count()['time_slot']\n",
    "    time.date = pd.to_datetime(time.date)\n",
    "    time.sort(['date', 'time_slot'], inplace =True)\n",
    "    return time\n",
    "\n",
    "def preparetimeSlotDistrict(data):\n",
    "    timeSlotDistrict = pd.concat([timeSlot for t in range(1,67)],ignore_index=True)\n",
    "    timeSlotDistrict['start_district_id']=[i for i in range(1,67) for j in range((timeSlot).shape[0])]\n",
    "    return timeSlotDistrict\n",
    "\n",
    "def prepareWeatherData(data, timeSlot, testData =False):\n",
    "    temp = data.copy(deep=True)\n",
    "    temp.time_slot = temp.time_slot.astype(int)\n",
    "    temp.drop(['Time'],axis = 1, inplace=True)\n",
    "    temp.drop_duplicates(['date', 'time_slot'], take_last = True, inplace=True)\n",
    "    temp = pd.merge(timeSlot, temp, on = ['time_slot', 'date'], how = 'left')\n",
    "    temp.fillna(method='ffill', limit=10, inplace =True)\n",
    "    temp.fillna(method='bfill', limit=10, inplace =True)\n",
    "    return temp \n",
    "\n",
    "def prepareTrafficData(data, timeSlotDistrict):\n",
    "    temp = data.copy(deep=True)\n",
    "    temp.time_slot = temp.time_slot.astype(int)\n",
    "    temp.district_id = temp.district_id.astype(int)\n",
    "    temp.sort(['district_id', 'Time'], inplace = True)\n",
    "    temp.drop(['Time'], axis = 1, inplace= True)\n",
    "    temp.rename(columns={'district_id':'start_district_id'}, inplace=True)\n",
    "    temp = pd.merge(timeSlotDistrict, temp, on = ['time_slot', 'date', 'start_district_id'], how = 'left')\n",
    "    \n",
    "    temp.sort(['date', 'start_district_id','time_slot'], inplace = True)\n",
    "    temp.fillna(method='bfill', limit=2, inplace =True)\n",
    "    temp = temp.fillna({'lv1':  int(temp.lv1.mean()), 'lv2':  int(temp.lv2.mean()), \n",
    "                        'lv3': int(temp.lv3.mean()), 'lv4': int(temp.lv4.mean())})\n",
    "    temp['lv1_pect'] = temp.lv1/(temp.lv1+temp.lv2+temp.lv3+temp.lv4) \n",
    "    temp['lv2_pect'] = temp.lv2/(temp.lv1+temp.lv2+temp.lv3+temp.lv4) \n",
    "    temp['lv3_pect'] = temp.lv3/(temp.lv1+temp.lv2+temp.lv3+temp.lv4) \n",
    "    temp['lv4_pect'] = temp.lv4/(temp.lv1+temp.lv2+temp.lv3+temp.lv4) \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timeSlot = timeSlotData(orders_train)\n",
    "#weather data process\n",
    "weather = prepareWeatherData(weather_train, timeSlot)\n",
    "temp = weather[weather.date == datetime.date(2016,1,21)]\n",
    "weather.fillna({'Weather': 4, 'PM25': int((temp.PM25.mean()+100)/2), 'Temperature': int(temp.Temperature.mean())}, inplace=True)\n",
    "print(weather.isnull().any())\n",
    "\n",
    "##traffic data process \n",
    "timeSlotDistrict = preparetimeSlotDistrict(timeSlot)\n",
    "traffic = prepareTrafficData(traffic_train, timeSlotDistrict)\n",
    "print(traffic.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepareOrderData(orders, weather, traffic, timeSlotDistrict):\n",
    "    index_col = ['date','time_slot', 'start_district_id']\n",
    "    grouped = orders.groupby(index_col)\n",
    "    data = pd.DataFrame()\n",
    "    data['demand']=grouped.count()['order_id']\n",
    "    data['supply']=grouped.count()['driver_id']\n",
    "    data = data.reset_index()\n",
    "    data['weekday'] = [t.isoweekday() for t in data.date]\n",
    "    data['gap']=data['demand']-data['supply']\n",
    "    data.date = pd.to_datetime(data.date)\n",
    "    data.start_district_id = data.start_district_id.astype(int)\n",
    "    data = pd.merge(timeSlotDistrict, data, on = ['time_slot', 'date', 'start_district_id'], how = 'left')\n",
    "\n",
    "    index_col = ['date','time_slot', 'start_district_id', 'min']\n",
    "    grouped = orders.groupby(index_col)\n",
    "    data_permin = pd.DataFrame()\n",
    "    data_permin['demand_per_min']=grouped.count()['order_id']\n",
    "    data_permin['supply_per_min']=grouped.count()['driver_id']\n",
    "    data_permin = data_permin.reset_index()\n",
    "    data_permin['weekday'] = [t.isoweekday() for t in data_permin.date]\n",
    "    data_permin['gap_per_min']=data_permin['demand_per_min']-data_permin['supply_per_min']\n",
    "    data_permin.date = pd.to_datetime(data_permin.date)\n",
    "    data_permin.start_district_id = data_permin.start_district_id.astype(int)\n",
    "    if 'weekday' in data_permin.columns:\n",
    "        data_permin.drop(['weekday'], axis= 1, inplace=True)\n",
    "    \n",
    "    total_data = data.copy(True)\n",
    "    total_data.date = pd.to_datetime(total_data.date)\n",
    "    total_data.start_district_id = total_data.start_district_id.astype(int)\n",
    "    col = ['date', 'gap', 'weekday', 'time_slot', 'start_district_id', 'demand', 'supply']\n",
    "    total_data = total_data[col]\n",
    "    key_col = ['date','time_slot']\n",
    "    total_data = pd.merge(total_data, weather, on = key_col, how = 'left' )\n",
    "    key_col = ['date','time_slot', 'start_district_id']\n",
    "    total_data = pd.merge(total_data, traffic, on = key_col, how = 'left')\n",
    "    \n",
    "    key_col = ['date','time_slot', 'start_district_id', 'min']\n",
    "    for i in range(1,11):\n",
    "        total_data['min'] = (total_data.time_slot-1)*10+i\n",
    "        total_data =  pd.merge(total_data, data_permin, on = key_col, how = 'left')\n",
    "        min_str = str(i)\n",
    "        col_dict = {'demand_per_min':'demand_min_'+ min_str, 'supply_per_min': 'supply_min_'+min_str, 'gap_per_min': 'gap_min_'+min_str}\n",
    "        total_data.rename(columns=col_dict, inplace=True)\n",
    "    \n",
    "    total_data.fillna(0, inplace = True)\n",
    "    total_data.sort(['date', 'start_district_id', 'time_slot'], inplace=True)\n",
    "    \n",
    "    #move t-1, t-2 and t-3 predictor to the same line in order to predict t0 gap \n",
    "    leftTable = total_data[['date', 'gap', 'weekday', 'time_slot', 'start_district_id']]\n",
    "    pass_1 = total_data.drop(['gap', 'weekday'], axis = 1)\n",
    "    pass_1['time_slot'] = pass_1['time_slot']+1\n",
    "    pass_2 = total_data.drop(['gap', 'weekday'], axis = 1)\n",
    "    pass_2['time_slot'] = pass_2['time_slot']+2\n",
    "    pass_3 = total_data.drop(['gap', 'weekday'], axis = 1)\n",
    "    pass_3['time_slot'] = pass_3['time_slot']+3\n",
    "\n",
    "    result = pd.merge(leftTable, pass_1, on = ['date', 'time_slot', 'start_district_id'], how = 'left')\n",
    "    result = pd.merge(result, pass_2, on = ['date', 'time_slot', 'start_district_id'], suffixes=('', '_t_2'), how = 'left')\n",
    "    result = pd.merge(result, pass_3, on = ['date', 'time_slot', 'start_district_id'], suffixes=('', '_t_3'), how = 'left')\n",
    "\n",
    "    result = result[(result.time_slot != 1) & (result.time_slot != 2) & (result.time_slot != 3) ]\n",
    "    result.drop(['min', 'date'], axis=1, inplace =True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = prepareOrderData(orders_train, weather, traffic, timeSlotDistrict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'train_data.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'traffic_train', traffic_train\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## direct read the small test set and training set from the pickle file\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "pickle_file = 'train_data.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_data = save['train_data']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('train_data', train_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
